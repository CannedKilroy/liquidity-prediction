{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf00b431-3e09-45d5-9e79-656b6d5e20b4",
   "metadata": {},
   "source": [
    "# Trade Aggregation\n",
    "### Date: May 20, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f5056-2ae3-4734-a0ee-fd2a0035e79f",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87437a6-92c4-42f4-8c7a-52c25d6bf941",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "<a id=\"Introduction\"></a>\n",
    "\n",
    "Quick overview: Cryptocurrency exchanges run off the taker-maker model, where every market order is matched to a limit order based on time price priority. The LOB holds all the limit ordes for each price level within a FIFO queue. When a market order is executed, it is matched with the first limit order in the queue at the best price, and the recorded in the time and sales. If the market order is still not filled, it will match with the second limit order at the best price, and so on. If it is still not fully executed at the best price, it will then start to match with the limit orders in the queue at the second best price, etc. \n",
    "\n",
    "So when a market order, that cannot be completely filled at the first trade at the first best price, is finished executing, the time and sales will have recorded dozens of partial fills. These partial fills are what we want to aggregate together. \n",
    "\n",
    "The only common informatiom that is shared between the partial fills websocket response, is that they hold the same trade direction and will have the same or very nearly same timestamp. We can look at the distribution of timestamp deltas to see if there are noticable gaps that might indicate an optimal window. Once a time window is found, we iterate through the data. Partial fills that are considered of the same trade are aggregated into a custom stack. If a partial fill does not meat the criteria to be considered part of the current trade, the stack is cleared, data aggregated, dumped to json, and appended to a parquet file. The next partial fill is then appended to the stack, and the process continues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8e157-58f4-44fc-bf7c-22ae11d1ee61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Table-of-contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Table Of Contents](#Table-of-contents)\n",
    "3. [Import Librarys](#Import-Librarys)\n",
    "4. [Data Dictionary](#Data-Dictionary)\n",
    "5. [Load Data](#Load-Data)\n",
    "   - [Trades Table](#Trades-Table)\n",
    "   - [Aggregation](#Aggregation)\n",
    "6. [Links](#Links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f98b9d-5bae-4f90-a89c-c70c94c9e143",
   "metadata": {},
   "source": [
    "## Import-Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf152d2-f046-4362-a147-b1cb3c61ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import json\n",
    "from scipy import stats\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "import mysql.connector\n",
    "import dask.dataframe as dd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7850a545-1333-4a4b-ab51-f210f15ed0c5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85fa98-3611-40d0-a9ee-de8731f2a14c",
   "metadata": {},
   "source": [
    "## Data-Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd2d389-a994-4be3-b2c6-cddc7c17bab4",
   "metadata": {},
   "source": [
    "The bybit websocket response docs:  \n",
    "https://bybit-exchange.github.io/docs/v5/websocket/public/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf00bdb-bf56-42b2-bff9-d4d30b2b1fed",
   "metadata": {},
   "source": [
    "## Load-Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d933167-8be8-4292-8b89-16f4b706e6ba",
   "metadata": {},
   "source": [
    "The first issue is how to work with a dataset this size without loading into memory. Pandas is not suitable for a dataset this side, so instead we will use Dask and Polars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6703224-7e09-49c2-b53a-e8311b29b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"data_crypto\"\n",
    "db_user = \"root\"\n",
    "db_password = \"root\"\n",
    "db_host = \"localhost\"\n",
    "\n",
    "uri = 'mysql+pymysql://root:root@localhost:3306/data_crypto'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5de863-de93-4bf7-aa9c-38ef7dc744a1",
   "metadata": {},
   "source": [
    "### Trades Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d951110-4de7-415e-9d98-399cb9abc58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"trades\"\n",
    "index_col = \"id\"\n",
    "\n",
    "engine = create_engine(uri)\n",
    "\n",
    "dask_df = dd.read_sql_table(\n",
    "    con=engine,\n",
    "    table_name=table_name,\n",
    "    uri=uri,\n",
    "    index_col=index_col,\n",
    "    npartitions=400 # Adjust as needed. This worked for 16gb memory\n",
    ")\n",
    "\n",
    "dask_df = dask_df.map_partitions(lambda df: df.sort_values(by='created_at'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465eda6-0862-4457-a9e0-9f1ab5366714",
   "metadata": {},
   "source": [
    "### Time Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ed231-da25-4736-a247-85d61c710f35",
   "metadata": {},
   "source": [
    "We can look at the distribution of time deltas to see if there are noticable gaps that might indicate an optimal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba47515-97a8-4195-b71d-3a64135c66fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "time_diffs = []\n",
    "trade_sides = []\n",
    "direction_match = []\n",
    "\n",
    "last = None\n",
    "\n",
    "for row in dask_df.itertuples(name=\"Trade\"):\n",
    "    new_info = ast.literal_eval(row.info)\n",
    "    \n",
    "    if last:\n",
    "        old_info = ast.literal_eval(last.info)\n",
    "        if last.trade_side == row.trade_side:\n",
    "            time_diff = new_info['T'] - old_info['T']\n",
    "            trade_sides.append(row.trade_side)\n",
    "            timestamps.append(new_info['T'])\n",
    "            time_diffs.append(time_diff)\n",
    "            direction_match.append(1)\n",
    "        else:\n",
    "            trade_sides.append(row.trade_side)\n",
    "            timestamps.append(new_info['T'])\n",
    "            time_diffs.append(0)\n",
    "            direction_match.append(0)\n",
    "    else:\n",
    "        trade_sides.append(row.trade_side)\n",
    "        timestamps.append(new_info['T'])\n",
    "        time_diffs.append(0)\n",
    "        direction_match.append(\"N/A\")\n",
    "        \n",
    "    last = row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44fc99-50ea-4a76-97bc-90d0be753c2e",
   "metadata": {},
   "source": [
    "Plot the dataframe to compare the timedeltas in trades that share the same direction as the previous trade vs those that switch directions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3496f-69d8-48cf-9cc4-1dd1920e3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Timestamp': pd.to_datetime(timestamps, unit='ms'),\n",
    "    'TimeDiff': time_diffs,\n",
    "    'TradeSide': trade_sides,\n",
    "    'DirectionMatch': direction_match\n",
    "})\n",
    "\n",
    "# Drop NA row\n",
    "df_filtered = df.drop(df.index[0])\n",
    "\n",
    "same_direction = df_filtered[df_filtered['DirectionMatch'] == 1]\n",
    "different_direction = df_filtered[df_filtered['DirectionMatch'] == 0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "ax1.hist(same_direction['TimeDiff'], bins=20, color='blue')\n",
    "ax1.set_title('Same Direction')\n",
    "ax1.set_xlabel('Time Difference (ms)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "ax2.hist(different_direction['TimeDiff'], bins=20, color='blue')\n",
    "ax2.set_title('Different Direction')\n",
    "ax2.set_xlabel('Time Difference (ms)')\n",
    "\n",
    "plt.suptitle('Histogram of Time Differences by Trade Direction Match')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469680b-a7c3-4973-8d90-b193e47fe529",
   "metadata": {},
   "source": [
    "There is no optimal time window to help separate same vs different direction trades. Interestingly only the different direction trades have low time deltas, where as same direction time deltas can be over a few seconds. This is likely due to the limited dataset. We can look if theres a time difference between buy and sell trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fbdad-ab94-47c3-a9d5-6294896cbc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_df = df[df['TradeSide'].str.lower() == 'buy']\n",
    "sell_df = df[df['TradeSide'].str.lower() == 'sell']\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.scatter(buy_df['Timestamp'], buy_df['TimeDiff'], color='blue', label='Buy', marker='o')\n",
    "plt.scatter(sell_df['Timestamp'], sell_df['TimeDiff'], color='red', label='Sell', marker='x')\n",
    "\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Time Difference (ms)')\n",
    "plt.title('Time Differences Between Trades')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567516a6-729b-465e-a564-25ed3480bb39",
   "metadata": {},
   "source": [
    "There is no noticable difference. There are times when the market is less active leading to larger time deltas between trades, and vise versa.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabd961-9b01-4aaa-ac87-40eea0eecfdd",
   "metadata": {},
   "source": [
    "Since there is no major difference by trade side or direction, we can look at all the time deltas on a lower timeframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b36d0-e029-4f28-adb0-6c47b31957a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA first row \n",
    "df_filtered = df.drop(df.index[0])\n",
    "\n",
    "df_filtered_20 = df[df['TimeDiff'] < 10]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "\n",
    "counts1, bins1, patches1 = ax1.hist(df['TimeDiff'], bins=50, edgecolor='black')\n",
    "ax1.set_xlabel('Time Difference (ms)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Histogram of Time Differences Between Trades')\n",
    "\n",
    "counts2, bins2, patches2  = ax2.hist(df_filtered_20['TimeDiff'], bins=50, edgecolor='black')\n",
    "ax2.set_xlabel('Time Difference (ms)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Histogram of Time Differences (Less than 10 ms) Between Trades')\n",
    "\n",
    "# Annotate\n",
    "bar_index = 0\n",
    "x = bins1[bar_index] + (bins1[bar_index + 1] - bins1[bar_index]) / 2  # Center\n",
    "y = counts1[bar_index]  # Height of the first bar\n",
    "annotation_text = \"Partial Fills\"\n",
    "ax1.annotate(annotation_text, (x, y), textcoords=\"offset points\", xytext=(50,0), ha='center', arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c888c9-85cf-48cb-8520-617a3c6ba270",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(df['TimeDiff'])\n",
    "median = np.median(df['TimeDiff'])\n",
    "mode = stats.mode(df['TimeDiff'])\n",
    "\n",
    "# Print results\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Mode:\", mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d63c8-c1f3-4f0b-9d84-04f56691d767",
   "metadata": {},
   "source": [
    "A time window of 0.4 ms should work as a good starting point. Partial fills of larger trades make up the majortity of the time and sales data, and would have by far the shortest changes in timestamp, being essentially instantanous with the ms timestamp accuracy provided by the websocket response. This means that the first bar is very likely all the partial fills we are trying to aggregate together, so we can expect a fairly accurate aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d955e7-84c9-48c5-a388-aee0e836d80f",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b1ca1-2e08-4678-b667-777513918ffe",
   "metadata": {},
   "source": [
    "Define custom aggregating class to aggregate and hold the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29222024-efa2-4dea-96ab-9694633dc0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStack(list):\n",
    "\n",
    "    def __init__(self, time_window):\n",
    "        self.time_window = time_window\n",
    "\n",
    "    def append(self, trade):\n",
    "        \"\"\"\n",
    "        Append an item to the stack only if it matches the pattern or stack is empty. \n",
    "        Returns None if successful append, othewise returns df of the aggregated data\n",
    "        \"\"\"\n",
    "\n",
    "        # If the stack is empty, append without checks\n",
    "        if not self:\n",
    "            super().append(trade)\n",
    "            return None\n",
    "\n",
    "        # Ignore trades that are not of the same exchange / symbol in case incoming data is mixed\n",
    "        if trade.exchange != self[-1].exchange or trade.symbol != self[-1].symbol:\n",
    "            return None\n",
    "            \n",
    "        # If trade is of different side or outside the time window\n",
    "        # to be considered part of the current stack, then \n",
    "        # aggregate, clear stack, and return data\n",
    "        if (trade.trade_side != self[-1].trade_side) or (trade.created_at - self[-1].created_at >= self.time_window):\n",
    "            data = self.aggregate()\n",
    "            self.clear()\n",
    "            return data\n",
    "\n",
    "        # If trade is of the same side, and in the time window, append\n",
    "        super().append(trade)\n",
    "    \n",
    "    def is_empty(self):\n",
    "        \"\"\"Check if the stack is empty\"\"\"\n",
    "        return not self\n",
    "\n",
    "    def top(self):\n",
    "        \"\"\"Return the most recent item at the top of the stack\"\"\"\n",
    "        if not self.is_empty():\n",
    "            return self[-1]\n",
    "        raise Exception(\"Stack is empty\")\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"Return the size of the stack\"\"\"\n",
    "        return len(self)\n",
    "    \n",
    "    def aggregate(self):\n",
    "        '''\n",
    "        Aggregate the trades. \n",
    "        '''\n",
    "        aggregates = {\"trade_ids\": [],\n",
    "                      \"executed_prices\": [],\n",
    "                      \"base_amounts\": [],\n",
    "                      \"costs\": [],\n",
    "                      \"infos\": [],\n",
    "                      \"date_times\": [],\n",
    "                      \"created_ats\": []}\n",
    "        \n",
    "        for trade in self:\n",
    "            aggregates[\"executed_prices\"].append(trade.executed_price)\n",
    "            aggregates[\"base_amounts\"].append(trade.base_amount)\n",
    "            aggregates[\"costs\"].append(trade.cost)\n",
    "            aggregates[\"infos\"].append(trade.info)\n",
    "            aggregates[\"date_times\"].append(trade.date_time.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            aggregates[\"created_ats\"].append(trade.created_at)\n",
    "        \n",
    "        for key, value in aggregates.items():\n",
    "            aggregates[key] = json.dumps(value)\n",
    "    \n",
    "        df = pd.DataFrame({\n",
    "        'exchange': self[-1].exchange,\n",
    "        'symbol': self[-1].symbol,\n",
    "        'trade_side': self[-1].trade_side,\n",
    "        'executed_price': aggregates['executed_prices'],\n",
    "        'base_amount': aggregates['base_amounts'],\n",
    "        'cost': aggregates['costs'],\n",
    "        'info': aggregates['infos'],\n",
    "        'date_time': aggregates['date_times'],\n",
    "        'created_at': aggregates['created_ats']\n",
    "        }, index =[0])\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74c6f6-9884-4ead-a35c-190702aebf78",
   "metadata": {},
   "source": [
    "Run the aggregation. This is very slow, and will take a few hours for a million rows, but is good enough for an initial dataset. Later on this can be optimized with async, less json serialization, using numpy or numba, possibly vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863a853-ddf1-4583-aa7d-bd0f393e62d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stack = CustomStack(1)\n",
    "file_name = 'aggregates'\n",
    "\n",
    "current_dir = Path(os.getcwd())\n",
    "print(\"Current directory:\", current_dir)\n",
    "\n",
    "project_root = current_dir.parents[0]\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "target_path = project_root / 'data' / 'processed'\n",
    "print(\"Target path for Parquet files:\", target_path)\n",
    "\n",
    "filepath = target_path\n",
    "print(\"Full file path:\", filepath)\n",
    "\n",
    "count = 0\n",
    "sorted_df = dask_df.sort_values('created_at').persist()\n",
    "\n",
    "for row in sorted_df.itertuples(name=\"Trade\"):\n",
    "    result = stack.append(row)\n",
    "\n",
    "    if result is not None:\n",
    "        print(row.Index)\n",
    "        ddf = dd.from_pandas(result, npartitions=1)\n",
    "        ddf.to_parquet(filepath.as_posix(), engine='pyarrow', write_index=False, append=True)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94631eb-34dc-4534-a4f1-0f2adf00173a",
   "metadata": {},
   "source": [
    "### Links\n",
    "- https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes#rough-rules-of-thumb\n",
    "- https://docs.dask.org/en/stable/best-practices.html\n",
    "- https://docs.dask.org/en/stable/dashboard.html\n",
    "- https://www.coiled.io/blog/reducing-dask-memory-usage\n",
    "- https://bicortex.com/data-analysis-with-dask-a-python-scale-out-parallel-computation-framework-for-big-data/\n",
    "- https://www.architecture-performance.fr/ap_blog/reading-a-sql-table-by-chunks-with-pandas/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
