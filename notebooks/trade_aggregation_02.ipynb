{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf00b431-3e09-45d5-9e79-656b6d5e20b4",
   "metadata": {},
   "source": [
    "# Trade Aggregation\n",
    "### Date: May 20, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f5056-2ae3-4734-a0ee-fd2a0035e79f",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87437a6-92c4-42f4-8c7a-52c25d6bf941",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "<a id=\"Introduction\"></a>\n",
    "\n",
    "This notebook aggregates the trades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8e157-58f4-44fc-bf7c-22ae11d1ee61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Table-of-contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Table Of Contents](#Table-of-contents)\n",
    "3. [Import Librarys](#Import-Librarys)\n",
    "4. [Data Dictionary](#Data-Dictionary)\n",
    "5. [Load Data](#Load-Data)\n",
    "   - [Trades Table](#Trades-Table)\n",
    "   - [Aggregation](#Aggregation)\n",
    "8. [Links](#Links)\n",
    "9. [Slippage Calculation and Analysis](#Slippage-Calculation-and-Analysis)\n",
    "10. [Implementation of Trading Strategies](#Implementation-of-Trading-Strategies)\n",
    "11. [Results and Discussion](#Results-and-Discussion)\n",
    "12. [Conclusion](#Conclusion)\n",
    "13. [References](#References)\n",
    "14. [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f98b9d-5bae-4f90-a89c-c70c94c9e143",
   "metadata": {},
   "source": [
    "## Import-Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf152d2-f046-4362-a147-b1cb3c61ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import json\n",
    "from scipy import stats\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import mysql.connector\n",
    "import dask.dataframe as dd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7850a545-1333-4a4b-ab51-f210f15ed0c5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85fa98-3611-40d0-a9ee-de8731f2a14c",
   "metadata": {},
   "source": [
    "## Data-Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd2d389-a994-4be3-b2c6-cddc7c17bab4",
   "metadata": {},
   "source": [
    "The bybit websocket response docs:  \n",
    "https://bybit-exchange.github.io/docs/v5/websocket/public/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf00bdb-bf56-42b2-bff9-d4d30b2b1fed",
   "metadata": {},
   "source": [
    "## Load-Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d933167-8be8-4292-8b89-16f4b706e6ba",
   "metadata": {},
   "source": [
    "The first issue is how to work with a dataset this size and combine both the `trades` and `orderbook` tables. Pandas is not suitable for a dataset this side, so instead we will use Dask and Polars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6703224-7e09-49c2-b53a-e8311b29b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"data_crypto\"\n",
    "db_user = \"root\"\n",
    "db_password = \"root\"\n",
    "db_host = \"localhost\"\n",
    "\n",
    "uri = 'mysql://root:root@localhost:3306/data_crypto'\n",
    "uri_2 = 'mysql+pymysql://root:root@localhost:3306/data_crypto'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5de863-de93-4bf7-aa9c-38ef7dc744a1",
   "metadata": {},
   "source": [
    "### Trades Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d951110-4de7-415e-9d98-399cb9abc58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"trades\"\n",
    "index_col = \"id\"\n",
    "\n",
    "engine = create_engine(uri_2)\n",
    "\n",
    "dask_df = dd.read_sql_table(\n",
    "    con=engine,\n",
    "    table_name=table_name,\n",
    "    uri=uri,\n",
    "    index_col=index_col,\n",
    "    npartitions=400 # Adjust as needed. This worked for 16gb memory\n",
    ")\n",
    "\n",
    "dask_df = dask_df.map_partitions(lambda df: df.sort_values(by='created_at'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465eda6-0862-4457-a9e0-9f1ab5366714",
   "metadata": {},
   "source": [
    "### Time Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ed231-da25-4736-a247-85d61c710f35",
   "metadata": {},
   "source": [
    "All crypto exchanges are maker-taker markets. Every market is matched with a limit order based on time price priority. The time and sales (also known as recent trades on some exchanges) lists these trades. When a large market order is executed, and cannot be filled completely at the best price, it matches with the second optimum price, and then 3rd, etc. These partial fills within each level is listed as a separate trade in the time and sales window. These are trades we want to aggregate together. \n",
    "\n",
    "The only info these partial fills share is the same trade direction and that there is a small time window between them. We can look at the distribution of time deltas to see if there are noticable gaps that might indicate an optimal window or if the distribution is fairly uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba47515-97a8-4195-b71d-3a64135c66fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "time_diffs = []\n",
    "trade_sides = []\n",
    "direction_match = []\n",
    "\n",
    "last = None\n",
    "\n",
    "for row in dask_df.itertuples(name=\"Trade\"):\n",
    "    new_info = ast.literal_eval(row.info)\n",
    "    \n",
    "    if last:\n",
    "        old_info = ast.literal_eval(last.info)\n",
    "        if last.trade_side == row.trade_side:\n",
    "            time_diff = new_info['T'] - old_info['T']\n",
    "            trade_sides.append(row.trade_side)\n",
    "            timestamps.append(new_info['T'])\n",
    "            time_diffs.append(time_diff)\n",
    "            direction_match.append(1)\n",
    "        else:\n",
    "            trade_sides.append(row.trade_side)\n",
    "            timestamps.append(new_info['T'])\n",
    "            time_diffs.append(0)\n",
    "            direction_match.append(0)\n",
    "    else:\n",
    "        trade_sides.append(row.trade_side)\n",
    "        timestamps.append(new_info['T'])\n",
    "        time_diffs.append(0)\n",
    "        direction_match.append(\"N/A\")\n",
    "        \n",
    "    last = row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44fc99-50ea-4a76-97bc-90d0be753c2e",
   "metadata": {},
   "source": [
    "Plot the dataframe to compare the timedeltas in trades that share the same direction as the previous trade vs those that switch directions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3496f-69d8-48cf-9cc4-1dd1920e3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Timestamp': pd.to_datetime(timestamps, unit='ms'),\n",
    "    'TimeDiff': time_diffs,\n",
    "    'TradeSide': trade_sides,\n",
    "    'DirectionMatch': direction_match\n",
    "})\n",
    "\n",
    "# Drop NA row\n",
    "df_filtered = df.drop(df.index[0])\n",
    "\n",
    "same_direction = df_filtered[df_filtered['DirectionMatch'] == 1]\n",
    "different_direction = df_filtered[df_filtered['DirectionMatch'] == 0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "ax1.hist(same_direction['TimeDiff'], bins=20, color='blue')\n",
    "ax1.set_title('Same Direction')\n",
    "ax1.set_xlabel('Time Difference (ms)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "ax2.hist(different_direction['TimeDiff'], bins=20, color='blue')\n",
    "ax2.set_title('Different Direction')\n",
    "ax2.set_xlabel('Time Difference (ms)')\n",
    "\n",
    "plt.suptitle('Histogram of Time Differences by Trade Direction Match')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469680b-a7c3-4973-8d90-b193e47fe529",
   "metadata": {},
   "source": [
    "There is no optimal time window it seems to help separate same vs different direction trades. Interestingly only the different direction trades have low time deltas, where as same direction time deltas can be over a few seconds. This is likely due to the limited dataset. We can look if theres a time difference between buy and sell trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fbdad-ab94-47c3-a9d5-6294896cbc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_df = df[df['TradeSide'].str.lower() == 'buy']\n",
    "sell_df = df[df['TradeSide'].str.lower() == 'sell']\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.scatter(buy_df['Timestamp'], buy_df['TimeDiff'], color='blue', label='Buy', marker='o')\n",
    "plt.scatter(sell_df['Timestamp'], sell_df['TimeDiff'], color='red', label='Sell', marker='x')\n",
    "\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Time Difference (ms)')\n",
    "plt.title('Time Differences Between Trades')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567516a6-729b-465e-a564-25ed3480bb39",
   "metadata": {},
   "source": [
    "There is no noticable difference except for at the end of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabd961-9b01-4aaa-ac87-40eea0eecfdd",
   "metadata": {},
   "source": [
    "Since there is no major difference by trade side or direction, we can look at all the time deltas on a lower timeframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b36d0-e029-4f28-adb0-6c47b31957a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA first row \n",
    "df_filtered = df.drop(df.index[0])\n",
    "\n",
    "df_filtered_20 = df[df['TimeDiff'] < 10]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "\n",
    "counts1, bins1, patches1 = ax1.hist(df['TimeDiff'], bins=50, edgecolor='black')\n",
    "ax1.set_xlabel('Time Difference (ms)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Histogram of Time Differences Between Trades')\n",
    "\n",
    "counts2, bins2, patches2  = ax2.hist(df_filtered_20['TimeDiff'], bins=50, edgecolor='black')\n",
    "ax2.set_xlabel('Time Difference (ms)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Histogram of Time Differences (Less than 10 ms) Between Trades')\n",
    "\n",
    "# Annotate\n",
    "bar_index = 0\n",
    "x = bins1[bar_index] + (bins1[bar_index + 1] - bins1[bar_index]) / 2  # Center\n",
    "y = counts1[bar_index]  # Height of the first bar\n",
    "annotation_text = \"Partial Fills\"\n",
    "ax1.annotate(annotation_text, (x, y), textcoords=\"offset points\", xytext=(50,0), ha='center', arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c888c9-85cf-48cb-8520-617a3c6ba270",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(df['TimeDiff'])\n",
    "median = np.median(df['TimeDiff'])\n",
    "mode = stats.mode(df['TimeDiff'])\n",
    "\n",
    "# Print results\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Mode:\", mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d63c8-c1f3-4f0b-9d84-04f56691d767",
   "metadata": {},
   "source": [
    "A time window of 0.4 ms should work as a good starting point. Partial fills of larger trades make up the majortity of the time and sales data, and would have by far the shortest changes in timestamp, being essentially instantanous with the ms timestamp accuracy. This means that the first bar is very likely all the partial fills we are trying to aggregate together, so we can expect a fairly accurate aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d955e7-84c9-48c5-a388-aee0e836d80f",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b1ca1-2e08-4678-b667-777513918ffe",
   "metadata": {},
   "source": [
    "Define custom aggregating class to aggregate and hold the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29222024-efa2-4dea-96ab-9694633dc0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStack(list):\n",
    "\n",
    "    def __init__(self, time_window):\n",
    "        self.time_window = time_window\n",
    "\n",
    "    def append(self, trade):\n",
    "        \"\"\"\n",
    "        Append an item to the stack only if it matches the pattern or stack is empty. \n",
    "        Returns None if successful append, othewise returns df of the aggregated data\n",
    "        \"\"\"\n",
    "\n",
    "        # If the stack is empty, append without checks\n",
    "        if not self:  \n",
    "            super().append(trade)\n",
    "            return None\n",
    "\n",
    "        # Ignore trades that are not of the same exchange / symbol in case incoming data has mixed data\n",
    "        if trade.exchange != self[-1].exchange or trade.symbol != self[-1].symbol:\n",
    "            return None\n",
    "            \n",
    "        # If trade is of different side or outside the time window\n",
    "        # to be considered part of the current stack, then \n",
    "        # aggregate, clear stack, and return data\n",
    "        if (trade.trade_side != self[-1].trade_side) or (trade.created_at - self[-1].created_at >= self.time_window):\n",
    "            data = self.aggregate()\n",
    "            self.clear()\n",
    "            return data\n",
    "        \n",
    "    def is_empty(self):\n",
    "        \"\"\"Check if the stack is empty\"\"\"\n",
    "        return not self\n",
    "\n",
    "    def top(self):\n",
    "        \"\"\"Return the most recent item at the top of the stack\"\"\"\n",
    "        if not self.is_empty():\n",
    "            return self[-1]\n",
    "        raise Exception(\"Stack is empty\")\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"Return the size of the stack\"\"\"\n",
    "        return len(self)\n",
    "    \n",
    "    def aggregate(self):\n",
    "        '''\n",
    "        Aggregate the trades. Exchange, symbol, \n",
    "        '''\n",
    "        trade_ids_json = json.dumps([trade.trade_id for trade in self])\n",
    "        executed_prices_json = json.dumps([trade.executed_price for trade in self])\n",
    "        base_amounts_json = json.dumps([trade.base_amount for trade in self])\n",
    "        costs_json = json.dumps([trade.cost for trade in self])\n",
    "        infos_json = json.dumps([ast.literal_eval(trade.info) for trade in self])\n",
    "        date_times_json = json.dumps([trade.date_time.strftime('%Y-%m-%d %H:%M:%S') for trade in self])\n",
    "        created_ats_json = json.dumps([trade.created_at for trade in self])\n",
    "    \n",
    "        df = pd.DataFrame({\n",
    "        'exchange': self[-1].exchange,\n",
    "        'symbol': self[-1].symbol,\n",
    "        'trade_id': trade_ids_json,\n",
    "        'trade_side': self[-1].trade_side,\n",
    "        'executed_price': executed_prices_json,\n",
    "        'base_amount': base_amounts_json,\n",
    "        'cost': costs_json,\n",
    "        'info': infos_json,\n",
    "        'date_time': date_times_json,\n",
    "        'created_at': created_ats_json\n",
    "        }, index =[0])\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74c6f6-9884-4ead-a35c-190702aebf78",
   "metadata": {},
   "source": [
    "Run the aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863a853-ddf1-4583-aa7d-bd0f393e62d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stack = CustomStack(0.5)\n",
    "file_name = 'aggregates'\n",
    "\n",
    "# Get the current working directory (assuming this is run in a Jupyter Notebook)\n",
    "current_dir = Path(os.getcwd())\n",
    "print(\"Current directory:\", current_dir)\n",
    "\n",
    "# Navigate two levels up from 'notebooks' to the 'liquidity' project root\n",
    "project_root = current_dir.parents[0]\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "# Define the path to the 'data/processed' directory\n",
    "target_path = project_root / 'data' / 'processed'\n",
    "print(\"Target path for Parquet files:\", target_path)\n",
    "\n",
    "# Define the full path for the Parquet file\n",
    "filepath = target_path\n",
    "print(\"Full file path:\", filepath)\n",
    "\n",
    "count = 0\n",
    "for row in dask_df.itertuples(name=\"Trade\"):\n",
    "    result = stack.append(row)\n",
    "\n",
    "    if result is not None:\n",
    "        print(row.Index)\n",
    "        ddf = dd.from_pandas(result, npartitions=1)\n",
    "        ddf.to_parquet(filepath.as_posix(), engine='pyarrow', write_index=False, append=True)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94631eb-34dc-4534-a4f1-0f2adf00173a",
   "metadata": {},
   "source": [
    "### Links\n",
    "- https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes#rough-rules-of-thumb\n",
    "- https://docs.dask.org/en/stable/best-practices.html\n",
    "- https://docs.dask.org/en/stable/dashboard.html\n",
    "- https://www.coiled.io/blog/reducing-dask-memory-usage\n",
    "- https://bicortex.com/data-analysis-with-dask-a-python-scale-out-parallel-computation-framework-for-big-data/\n",
    "- https://www.architecture-performance.fr/ap_blog/reading-a-sql-table-by-chunks-with-pandas/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
